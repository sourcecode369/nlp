## Transformers Tutorials

### Introduction
New deep learning model are introduced at an increasing rate and sometimes it’s hard to keep track of all the novelties. That said, one particular neural network model has proven to be especially effective for common natural language processing tasks. The model is called a Transformer and it makes use of several methods and mechanisms. The paper ‘Attention Is All You Need’ introduces a novel architecture called Transformer. As the title indicates, it uses the attention-mechanism.

<p align='center'><img src='https://miro.medium.com/max/700/1*BHzGVskWGS_3jEcYYi6miQ.png' height=450 width=400></p>

The field of **NLP** was revolutionized in the year 2018 by introduction of **BERT** and his **Transformer** friends(RoBerta, XLM etc.).

These novel transformer based neural network architectures and new ways to training a neural network on natural language data introduced transfer learning to NLP problems. Transfer learning had been giving out state of the art results in the Computer Vision domain for a few years now and introduction of transformer models for NLP brought about the same paradigm change in NLP.

[Hugging Face](https://github.com/huggingface) made it easier for community to access and fine tune these models using their Python Package: [Transformers](https://github.com/huggingface/transformers). 
As the huggingface [documentation](https://huggingface.co/transformers/) says,
> *Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.*

### Motivation
The implementation of these notebooks and scripts have one and only sole purpose i.e. better, simple understanding and implementation aspects of fine tuning of these language models on various NLP tasks. Despite these amazing technological advancements applying these solutions to business problems is still a challenge given the niche knowledge required to understand and apply these method on specific problem statements. In the following tutorials i will be demonstrating how a user can leverage technologies along with some other python tools to fine tune these Language models to specific type of tasks. 

### Resources and Further Reading
You can improve your knowledge on this topic by reading/watching the following resources:

### References:

#### General 
1. https://notebooks.quantumstat.com/
2. https://www.kaggle.com/
3. https://keras.io/examples/
4. https://www.tensorflow.org/tutorials/text/word_embeddings
5. https://pytorch.org/tutorials/beginner/transformer_tutorial.html

#### NLP Repositories
1. https://github.com/graykode/nlp-tutorial 

#### Blogs 
##### Transformers
1. https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec
2. http://peterbloem.nl/blog/transformers
3. https://nlp.seas.harvard.edu/2018/04/03/attention.html#additional-components-bpe-search-averaging
4. https://mlexplained.com/2019/07/04/building-the-transformer-xl-from-scratch/
