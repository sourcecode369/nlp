# -*- coding: utf-8 -*-
"""Preparing NLI dataset using nlp Lib.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SAXTqHQyvhpnL3MOxHrm6YZ9V-SF_M1Z
"""

!pip install nlp 
!pip install tokenizers
!pip install transformers

import os
import torch

import nlp
import tokenizers
import transformers

mnli = nlp.load_dataset(path='glue', name='mnli', split='train[:50%]')

from transformers import XLMRobertaTokenizer
tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')

def convert_to_features(batch):
    input_pairs = list(zip(batch['premise'], batch['hypothesis']))
    encodings = tokenizer.batch_encode_plus(input_pairs, 
                                            add_special_tokens=True, 
                                            padding=True, 
                                            max_length=96, 
                                            truncation=True, 
                                            return_attention_mask=True, 
                                            return_token_type_ids=True)
    return encodings

"""### MNLI"""

mnli_encoded_dataset = mnli.map(convert_to_features, batched=True, remove_columns=['idx', 'premise', 'hypothesis'])
mnli_encoded_dataset.set_format("torch", columns=['attention_mask', 'input_ids', 'token_type_ids', 'label'])

print(mnli_encoded_dataset.num_rows)
print(mnli_encoded_dataset.num_columns)
print(mnli_encoded_dataset.column_names)

"""### XNLI"""

xnli = nlp.load_dataset(path='xnli')
xnli = nlp.concatenate_datasets([xnli['test'], xnli['validation']])

def preprocess_xnli(example):
    premise_output = []
    hypothesis_output = []
    label_output = []
    for prem, hyp, lab in zip(example['premise'],  example['hypothesis'], example["label"]):
        label = lab
        langs = hyp['language']
        translations = hyp['translation']
        hypothesis = {k: v for k, v in zip(langs, translations)}
        for lang in prem:
            if lang in hypothesis:
                premise_output += [prem[lang]]
                hypothesis_output += [hypothesis[lang]]
                label_output += [label]
    return {'premise':premise_output, 'hypothesis':hypothesis_output, 'label':label_output}

xnli_processed = xnli.map(preprocess_xnli, batched=True).shuffle(seed=2020)
xnli_encoded = xnli_processed.map(convert_to_features, batched=True, remove_columns=['premise', 'hypothesis'])
xnli_encoded.set_format("torch", columns=['attention_mask', 'input_ids', 'token_type_ids', 'label'])

print(xnli_processed.num_rows)
print(xnli_processed.num_columns)
print(xnli_processed.column_names)

print(xnli_encoded.num_rows)
print(xnli_encoded.num_columns)
print(xnli_encoded.column_names)

"""## The Stanford Natural Language Inference Corpus (SNLI)"""

snli = nlp.load_dataset(path='snli', split='train[:20%]')

print(len(snli.filter(lambda x: x['label']==0)))
print(len(snli.filter(lambda x: x['label']==1)))
print(len(snli.filter(lambda x: x['label']==2)))

snli_encoded = snli.map(convert_to_features, batched=True, remove_columns=['premise', 'hypothesis'])
snli_encoded.set_format("torch", columns=['attention_mask', 'input_ids', 'token_type_ids', 'label'])

"""### Check all three datasets"""

mnli_encoded_dataset.num_rows + snli_encoded.num_rows + xnli_encoded.num_rows

print(mnli_encoded_dataset.column_names)
print(snli_encoded.column_names)
print(xnli_encoded.column_names)

dataset = nlp.concatenate_datasets([mnli_encoded_dataset, snli_encoded, xnli_encoded])

dataset = dataset.shuffle(seed=2020)

